{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "from processed_feature_mapping import mapping\n",
    "from train_model import data_cleaning\n",
    "from model_monitoring import ModelMonitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a. Output the required cleaned data for train dataset\n",
    "# Required processed data is already done in train_model.py\n",
    "# Although prediction on employee_train was already done previously,\n",
    "# need to do again because the dataset was jumbled up in train_model.py\n",
    "train_df = pd.read_csv('../data/raw_split_data/employee_train.csv')\n",
    "train_df = data_cleaning(train_df)\n",
    "X_train = train_df.drop(columns=['Attrition'])\n",
    "\n",
    "column_transformer = load('./preprocessor/column_transformer.pkl')\n",
    "label_encoder = load('./preprocessor/label_encoder.pkl')\n",
    "RF_clf = load('./model/RF_clf.joblib')\n",
    "\n",
    "y_train = train_df['Attrition']\n",
    "X_train_processed = column_transformer.transform(X_train)\n",
    "y_train_pred = RF_clf.predict(X_train_processed)\n",
    "y_train_pred_inverse = label_encoder.inverse_transform(y_train_pred)\n",
    "train_df['prediction'] = y_train_pred_inverse\n",
    "train_df.rename(columns={'Attrition' : 'target'}, inplace=True)\n",
    "train_df.to_csv('../data/cleaned_employee_train.csv', index=False)\n",
    "\n",
    "# 1b. Output the required cleaned/processed data for test dataset\n",
    "test_df = pd.read_csv(\"../data/raw_split_data/employee_test.csv\")\n",
    "test_df = data_cleaning(test_df)\n",
    "X_test = test_df.drop(columns=['Attrition'])\n",
    "y_test = test_df['Attrition']\n",
    "X_test_processed = column_transformer.transform(X_test)\n",
    "X_test_processed = pd.DataFrame.from_records(X_test_processed)\n",
    "X_test_processed = mapping(X_test_processed, column_transformer)\n",
    "X_test_processed.to_csv('../data/X_test_processed.csv', index=False)\n",
    "y_test_pred = RF_clf.predict(X_test_processed)\n",
    "y_test_pred_inverse = label_encoder.inverse_transform(y_test_pred)\n",
    "y_test_pred_prob = RF_clf.predict_proba(X_test_processed)[:1]\n",
    "test_df['prediction'] = y_test_pred_inverse\n",
    "test_df.rename(columns={'Attrition' : 'target'}, inplace=True)\n",
    "test_df.to_csv('../data/cleaned_employee_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the required datasets out, firstly, we create the instance of the ModelMonitoring object \n",
    "train_df = pd.read_csv('../data/cleaned_employee_train.csv')\n",
    "test_df = pd.read_csv('../data/cleaned_employee_test.csv')\n",
    "processed_train_df = pd.read_csv('../data/X_train_processed.csv')\n",
    "processed_test_df = pd.read_csv('../data/X_test_processed.csv')\n",
    "model_monitoring = ModelMonitoring(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first have to ensure that the incoming data has to issues\n",
    "for example, that the column names of the incoming data matches the training data, \n",
    "there is no difference in the data type for all columns, \n",
    "there is no data with '?' or '-' within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test dataset have the same features\n",
      "The data types for all columns from both datasets are the same\n",
      "train and test dataset have the same processed features\n"
     ]
    }
   ],
   "source": [
    "# so we first have to run the data_check method\n",
    "# this method will fix any column names that has spaces and change them to '_'\n",
    "# it then replaces any bad data such as '?' or '-' and replaces the with NaN\n",
    "# it then checks whether the schema of the incoming data matches the training data\n",
    "# then it checks for the data types for each column of the incoming data\n",
    "# lastly it also checks that the processed incoming data is the same as the processed training data\n",
    "# this is to ensure the data quality of the incoming data and that the\n",
    "# processing pipeline has no issues\n",
    "model_monitoring.data_check(train_df, test_df, processed_train_df, processed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
